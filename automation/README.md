# Orcha-1 Automation

LLM-powered automation for the Orcha-1 FoodTec integration, enabling natural language processing of food orders.

## Directory Structure

```
automation/
â”œâ”€â”€ requirements.txt        # Python deps
â”œâ”€â”€ README.md               # Docs for automation module
â”œâ”€â”€ main.py                 # Entry point for orchestration server/CLI
â”œâ”€â”€ orchestrator/           # Core orchestration logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ llm.py              # LLM wrapper (LangChain/OpenAI)
â”‚   â”œâ”€â”€ planner.py          # Plans steps: export â†’ validate â†’ accept
â”‚   â”œâ”€â”€ runner.py           # Executes plan against MCP
â”‚   â””â”€â”€ schemas.py          # Shared schemas for payloads
â”œâ”€â”€ clients/                # MCP/Proxy clients
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mcp_client.py       # JSON-RPC client for MCP (9090)
â”‚   â””â”€â”€ proxy_client.py     # JSON-RPC client for Proxy (8080)
â””â”€â”€ tests/
    â”œâ”€â”€ test_llm.py
    â”œâ”€â”€ test_runner.py
    â””â”€â”€ test_e2e.py
```

## Setup

1. Install dependencies:

```bash
cd automation
pip install -r requirements.txt
```

2. Set OpenAI API Key:

```bash
# Linux/Mac
export OPENAI_API_KEY=your-key-here

# Windows PowerShell
$env:OPENAI_API_KEY="your-key-here"
```

## Usage

### Starting the Automation CLI

Make sure your Orcha-1 stack is running:

1. Start Proxy: `cd proxy && python main.py` (or `uvicorn proxy.main:app --reload --port 8080`)
2. Start MCP: `cd MCP && node dist/index.js`
3. Start Automation CLI: `cd automation && python main.py`

### Using the CLI

Once the CLI is running, you can interact with it:

```
ðŸ¤– Orcha-1 Automation CLI
Type 'exit' or 'quit' to end the session

ðŸ‘¤ You: show me the menu
ðŸ“¦ Planner chose tool: foodtec.export_menu
MCP Response: { ... menu data ... }

ðŸ‘¤ You: place an order
ðŸ“¦ Planner chose tool: foodtec.validate_order
MCP Response: { ... validation response ... }

ðŸ‘¤ You: confirm my order
ðŸ“¦ Planner chose tool: foodtec.accept_order
MCP Response: { ... acceptance response ... }
```

## Testing

### Unit Tests

```bash
pytest tests/test_llm.py tests/test_runner.py
```

### End-to-End Tests

To run E2E tests that interact with the actual MCP server:

```bash
# Enable E2E tests
export RUN_E2E_TESTS=1

# Run the tests
pytest tests/test_e2e.py
```

## LLM Integration

This module uses LangChain with OpenAI's models to process natural language orders. The LLMWrapper class encapsulates the language model interactions, making it easy to switch between different models or providers.

### Planner Logic

The planner uses LLM to determine which action to take:

1. For menu requests â†’ `foodtec.export_menu`
2. For order confirmation â†’ `foodtec.validate_order`
3. For final orders â†’ `foodtec.accept_order`

The prompt structure ensures consistent decision-making based on user intent.

## Integration with MCP UI

The Automation component can be integrated with the MCP UI by:

1. Creating a simple REST API using FastAPI or Flask
2. Adding endpoints for the UI to call
3. Implementing WebSocket support for real-time updates

Details on this integration can be found in `docs/ui_automation_integration.md`

## License

This project is licensed under the MIT License - see the LICENSE file for details.
